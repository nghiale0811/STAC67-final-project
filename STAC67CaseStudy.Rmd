---
title: "STAC67 Case Study Project: Analyzing The Factors That Affect Risk of Cervical Cancer Relapse"
subtitle: "Project Group #13"
output: pdf_document
---

\vspace{250pt}

# Team Members:

## Joshua Garvida (1003042520)
Background and Significance, Discussion/Conclusion, References

## Jimmy Deng (1006280472)
Exploratory Data Analysis, Graphs/Visuals

## Daniyal Iqbal (1006145805)
Model Selection and Checking, Data Handling

## Nghia Le (1007523958)
Model Selection, Model Diagnostics, ML Models

\newpage

### Introduction

The initial concept of a bicycle sharing system started in 1965 in Amsterdam (Shaheen, Guzman, & Zhang, 2010), with the intent to better environmental and social welfare. Through systems in which people rent a bicycle at minimal costs at one docking station and return it in another, bicycle sharing as a whole has significant impacts in “increasing the use of transportation, minimizing greenhouse gas emissions, enhancing public health and also traffic troubles.” (Park & Cho, 2020) The introduction of rental bikes has shown tremendous growth as there are more than 50 countries implementing this kind of system, informed by the renting of bicycles being cost effective and at certain instances being free of charge. However, a problem arises as the constant rise of users means a higher demand for accessible bicycles. Therefore, there is a need for the supply of rental bikes to match this demand to ensure accessibility and decrease wait times. This is the case for “Ddareungi,” or “Seoul Bike,” the bicycle sharing system in South Korea started in 2015. Through the “Seoul Bike” data and understanding the significance of various weather information, our research aims to estimate the optimal amount of rental bikes needed at each hour of the day. 

```{r include=FALSE, message=FALSE, warning=FALSE, results="hide"}
library(tidyverse)
library(readxl)
library(ggcorrplot)
library(broom)
library(MASS)
library(car)
library(lmtest)
library(olsrr)
library(corrr)
library(gridExtra)
library(grid)
library(pander)
```

## EXPLORATORY DATA ANALYSIS
#### Description of dataset
The data set used in this case study comes from data.Seoul.kr as part of data collection done by the Seoul Metropolitan Government between January 2017 and November 2018 on several variables of data points for predicting supply of rental bikes per hour. The data set is comes in an excel file with 14 variables and 8760 observations.

```{r}
bike <- read_excel("./SeoulBikeData.xlsx")    # Reading in the data set
```

The 14 variables in the data set include dates, quantitative variables, and qualitative variables. The quantitative variables include, rented bike count, hour, temperature, humidity, wind speed, visibility, dew point temperature, solar radiation, rainfall, and snowfall. The qualitative variables include, seasons, holiday, and functioning day, where the latter two are binary variables dictating whether that day is a holiday/functioning day or not and the former indicates the season it is. The identifier for each observation consists of the date and hour. The response variable in this data set is Rented Bike Count and the rest are explanatory variables.

#### Extracting and cleaning data
For this data set, we chose discarded date as the sole identifier and kept hour for our analysis because the hour of day can have a more observable effect on the response compared to specific days in the year (as seen later in graph analysis). Furthermore, we decided to keep hour as a quantitative variable instead of qualitative due to time having a set order. Further analysis may include visualizing and exploring this data set as a times series, but was forgone in this case study for simplicity.
```{r, echo=FALSE}
# Response variable
rbc <- bike$`Rented Bike Count`
# Explanatory variables
# Quantitative variables
hour <- bike$Hour
temp <- bike$`Temperature(°C)`
humid <- bike$`Humidity(%)`
windspd <- bike$`Wind speed (m/s)`
vis <- bike$`Visibility (10m)`
dptemp <- bike$`Dew point temperature(°C)`
solrad <- bike$`Solar Radiation (MJ/m2)`
rain <- bike$`Rainfall(mm)`
snow <- bike$`Snowfall (cm)`
# Categorical variables
season <- as.numeric(str_replace_all(bike$Seasons, c("Winter" = "1", "Spring" = "2",
                                                     "Summer" = "3", "Autumn" = "4")))
holiday <- as.numeric(str_replace_all(bike$Holiday, c("No Holiday" = "0", "Holiday" = "1")))
funcday <- as.numeric(str_replace_all(bike$`Functioning Day`, c("Yes" = "1", "No" = "0")))
```

#### Plotting individual variables
With the data imported, we can begin with taking an overview of the data by plotting the individual variables to illustrate the general distributions of our data. 
```{r, fig.width=10, out.height="30%", echo=FALSE}
# NOTE: the plotting techniques used here come from Dr. Ken Butler's course STAC33 and textbook PASIAS
# Combining columns of quantitative variables to be plotted together
cont_vars <- bike %>% pivot_longer(`Rented Bike Count`:`Snowfall (cm)`,
                                   names_to = "xname", values_to = "x")
# Combining columns of categorical variables to be plotted together
cat_vars <- bike %>% 
  dplyr::select(`Rented Bike Count`, Seasons, Holiday, `Functioning Day`) %>%
  pivot_longer(-`Rented Bike Count`, names_to = "xname", values_to = "x") 
# Continuous variables
# Histogram plots
ggplot(cont_vars, aes(x = x)) +
  geom_histogram(fill = "slategray4", color = "white", bins = 24) +
  facet_wrap(~xname, scales = "free") + labs(title = "Histograms of Continous Variables")
# Box plots
ggplot(cont_vars, aes(x = x)) +
  geom_boxplot(fill = "slategray4") + facet_wrap(~xname, scales = "free") +
  labs(title = "Boxplots of Continous Variables")
```

Here, it is seen that most of the quantitative variables are skewed in some fashion. Notably, rainfall, snowfall, solar radiation, wind speed have extreme right skew distributions, and visibility has an extreme left skew. Hour has a uniform distribution, and the response variable has a right skew distribution. Humidity, dew point temperature, and temperature have minor skews in their distribution.

```{r fig.width=10, out.height="30%", echo=FALSE}
# Categorical variables
# Bar plot
ggplot(cat_vars, aes(x = fct_inorder(x))) +
  geom_bar(fill = "slategray4") + facet_wrap(~xname, scales = "free") +
  labs(title = "Bar plots of Categorical Variables", x = "factors")
```

For the categorical variables, season appears to be uniformly distributed and the other two have extreme right skews.

```{r, echo=FALSE}
# NOTE: Data frame manipulation techniques come from Dr. Ken Butler's course STAC33 and textbook PASIAS
# Setting options for pander rendering of tables
panderOptions("table.split.table", Inf)
# Function for finding mode of a vector
mode <- function(x) {
  unique_val <- unique(x)    # Get vector of unique values in x
  max_val <- which.max(tabulate(match(x, unique_val)))    # Get max occurrence of each unique value in x
  m <- unique_val[max_val]     # Get value with max occurrences
  return(m)
}
# Some statistics of these variables
# Summary statistics table for quantitative variables
bike %>% dplyr::select(-c(Date, Seasons, Holiday, `Functioning Day`)) %>%    # Take all quantitative variables
  pivot_longer(everything(), names_to = "variable", values_to = "x") %>%     # Combine their values sorted by their column
  group_by(variable) %>%
  summarize(mean = mean(x), median = median(x), mode = mode(x),sd = sd(x), var = var(x),    # Create summary table
            Q1 = quantile(x, 0.25),Q3 = quantile(x, 0.75), max = max(x), min = min(x)) %>%
              mutate_at(2:9,round, 2) -> summary_cont
pander(summary_cont)
```

Here we have generated a summary table of some statistics of the quantitative variables. For some of the extreme skew distributions from before, we can see that some of the quartile values correspond with the minimum such as rainfall/snowfall. These zero values in our data may not be very helpful in predicting our response value and could be considered for removal in our model. Likewise for visibility, most values seem to correspond with the maximum, which means a possible limit in measurement of data. This limit could be obscuring some trends past value of 2000 and may also be considered for removal.

#### Plotting response variable against others
Now that we've made some observations on the individual variables themselves, we will plot these variables against the Rented Bike Count to see how their values relate to the number of bikes rented.

```{r, fig.width=10, echo=FALSE}
# NOTE: the plotting techniques used here come from Dr. Ken Butler's course STAC33 and textbook PASIAS
# Combining quantitative variables with Rented Bike Count from bike to be plotted against
cont_vars_rbc <- bike %>% pivot_longer(`Hour`:`Snowfall (cm)`, names_to = "xname", values_to = "x")
# Cont. vars vs rbc
# Scatter plot
cont_vars_rbc %>% ggplot(aes(x = x, y = `Rented Bike Count`)) + geom_point(colour = "slategray4") +
  facet_wrap(~xname, scales = "free") + labs(title = "Scatterplots of Continous Variables against Response")
```

From these plots, we can see that some variables have linear trends such as temperature, and dew point temperature. Other variables have non-linear trends, which may be better fitted with some polynomial or logarithmic methods later on.

```{r fig.width=10, out.height="30%", echo=FALSE}
# Cat. vars vs rbc
# Box plot
cat_vars %>% ggplot(aes(x = fct_inorder(x), y = `Rented Bike Count`)) + geom_boxplot(fill = "slategray4") +
  facet_wrap(~xname, scales = "free") + labs(title = "Boxplots of Categorical Variables against Response", x = "factors")
```

For categorical variables, we can see how different factors affect the rented bike count. Some observations are, rented bike count is always 0 for non-functioning day, and bike count is higher in warmer months.

#### Plotting correlation between explanatory variables
After looking and noticing some observations from plots, we can further explore relationships in our model my looking at correlation between the variables and the response.
```{r out.height="25%", echo=FALSE}
# For quantitative variables
correlation_matrix <- cor(cbind(hour, temp, humid, windspd, vis, dptemp, solrad, rain, snow))
pander(round(correlation_matrix, 3))
# Correlation matrix
ggcorrplot(correlation_matrix, title = "Correlation Matrix of continuous Explanatory Variables")
# Abs. Corr. matrix
ggcorrplot(abs(correlation_matrix), title = "Absolute values for Correlation Matrix")
```

From correlation matrix and heat map, we can see signs of multicolinearity from high correlation between dew point temperature and temperature. Note that humidity has noticeably moderate values of correlation with a few other variables, which we should keep in mind although it is not high enough to be worrying. Additionally, these values for humidity are negative, meaning that humidity appears to inversely proportional these other variables.

We can also look at how quantitative variables correlate to rented bike count.
```{r echo=FALSE, message=FALSE, warning=FALSE, out.width="50%"}
# Correlation to response
cbind(rbc, hour, temp, humid, windspd, vis, dptemp, solrad, rain, snow) %>% 
  correlate() %>% focus(rbc) %>% arrange(desc(rbc)) %>%
  rename(Correlation = "rbc") -> corr_resp
# Heat map for correlation to response
rbc_corr_map <- ggplot(corr_resp %>% mutate(corr = "rbc"), aes(x= corr, y = fct_inorder(term), fill = Correlation)) + geom_tile() + scale_y_discrete(limits=rev) + labs(title = "Correlation between Continuous Variables and Response", y = "variable", x = "response")
# Arranged output for paper with table next to plot
rbc_corr_map
grid.newpage()
grid.table(corr_resp)
```

Correlation between continuous explanatory variables and response shows that temperature is the most correlated value. Lowest correlation is between rain, snow, and humidity. The takeaway here is that the higher the correlation between rented bike count and the variable is, the more positive contribution that variable gives to the value of rented bike count. With this in mind, variables like temperature and hour will help predict the rented bike count best, while variables at the bottom with negative correlation, will predict against the response, or will decrease the value of rented bike count.
t(correlation_matrix)

# MODEL BUILDING

We start by building a _main effect_ model, that includes all the explanatory variables\ 
We regress the response variable against the variables to have a sense of how our model behaves intially.

```{r, echo=FALSE}
model1 <- lm(`Rented Bike Count` ~ .-Date, data = bike)
summary(model1)
```

Based on the produced output we can see the relationship with the response and explaintory variables is\ 
about __55%__. We begin to test individual variables against the hypothesis of that estimate being irrelavant.\ 
We also notice multi-collinearity as shown by the output in the above graphs. We will remove those variables in\ 
hopes of improving our relationship and make our model simpler leading to a higher predictive function.

```{r, echo=FALSE}
model2 <- lm(`Rented Bike Count` ~ .-Date-`Humidity(%)`, data = bike)
summary(model2)
```

After one of the highly correlated variables we see a small adjustment in $R^2$ which is not a significant\ 
amount to come to a solid conclusion, so we will keep removing correlated variables till we see an improvement\ 
or we run out of correlated variables to remove from our model.

```{r, echo=FALSE}
model2.1 <- lm(`Rented Bike Count` ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`, data = bike)
model2.1$call
summary(model2.1)$r.squared
summary(model2.1)$adj.r.squared
```
Now solrad and snow have become insignificant, and R-squared values dropping by 0.02. We would drop solrad first, then check and drop snowfall if needed

```{r, echo=FALSE}
model2.2 <- lm(`Rented Bike Count` ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`, data = bike)
model2.2$call
summary(model2.2)$r.squared
summary(model2.2)$adj.r.squared

model2.3 <- lm(`Rented Bike Count` ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`, data = bike)
model2.3$call
summary(model2.3)$r.squared
summary(model2.3)$adj.r.squared
```
After removing all the correlated variables, we still do not obtain a decent relationship\ 
between the response and explanatory variables. We draw residual plots to analyse how the\ 
errors are scattered and to further check if our regression assumptions are being met \ 
in hopes to create a model that will improve our relationship and hence create a decent\ 
predictive model.

```{r include=FALSE}
# Add values from data frame into linear model for plotting against residuals
model2.3 %>% augment(bike) -> model2.3a

# Modifying data frame for plotting exp. vs resid. using facet wrap
model2.3a_long <- model2.3a %>% pivot_longer(c(Hour,`Temperature(°C)`, `Wind speed (m/s)`, `Visibility (10m)`, `Rainfall(mm)`), values_to = "x", names_to = "xname")
```

```{r, out.height="30%"}
# Fitted vs residuals
ggplot(model2.3, aes(x = .fitted, y = .resid)) + geom_point()

# Explanatory vs residuals
#ggplot(model2.3a_long, aes(x = x, y = .resid)) + geom_point() + facet_wrap(~xname, scales = "free")

# QQ plot against residuals
ggplot(model2.3, aes(sample = .resid)) + stat_qq() + stat_qq_line()


bptest(model2.3)    # Breusch-Pagan test, reject constant variance
```

The most apparent observation we notice is that our data violates the normality assumption \ 
this is further apparent via the __BP__ test. Another observation we make is this interesting \ 
linear line of residuals in our residual plot, giving us the possible conclusion we are missing\ 
some important variable(s) that would bridge the gap of the poor relationship between the response\ 
and explanatory variables. Since our normality assumptions have been violated a quick remedy is to\ 
apply a box-cox transformation to transform the data to satisfy the normality assumption.\ 


```{r, echo=FALSE, out.width="30%"}
# Might remove this b/c it makes our model worse lol
# Use boxCox(model2.3, family="yjPower") w/ a$x[which.max(a$y)]

# YJ Transformation, Boxcox w/ neg. vals.
#biket <- bike %>% mutate(`Rented Bike Count` = yjPower(bike$`Rented Bike Count`, 2))

model3 <- lm(I(`Rented Bike Count`^0.1818) ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`, data = bike)
model3$call
summary(model3)$r.squared
summary(model3)$adj.r.squared

# Fitted vs residuals
ggplot(model3, aes(x = .fitted, y = .resid)) + geom_point()

# QQ plot against residuals
ggplot(model3, aes(sample = .resid)) + stat_qq() + stat_qq_line()
```
```{r, echo=FALSE}
rf = rstandard(model2.3)
s.hat = abs(rf)
rf2 = lm(s.hat~.-Date, data=bike[-c(2)])
var.s = (predict(rf2))^2

model3b <- lm(`Rented Bike Count` ~ .-Date, data = bike, weight = 1/var.s)
model3b$call
summary(model3b)$r.squared
summary(model3b)$adj.r.squared
```
This transformation seems more inline with a good model, but we still have some outliers\ 
and this weird line on the left. Another solution is to you __weighted least squares (WLS)__ \ 
which normalizes the data and fixes normality assumptions and constant variance violations.\ 
We have tried WLS, but it doesn't help. Doing Box-Cox or Yeo-Johnson yields better results \ 
at this moment.

## FEATURE IMPORTANCE USING CARET
```{r, include=FALSE}
set.seed(222)
library(caret)
library(mlbench)
```

```{r}
importance <- varImp(model1, scale=FALSE)
print(importance)

model6 <- lm(I(`Rented Bike Count`^0.1818) ~ .-Date-`Temperature(°C)`-`Dew point temperature(°C)`-`Visibility (10m)`-`Snowfall (cm)`-`Wind speed (m/s)`, data = bike)
model6$call
summary(model6)$r.squared
summary(model6)$adj.r.squared
```

__Extension:__ We used a feature a selection algorithm which gives us possible\ 
important features for our model building process. Using the features specified by \ 
the algorithm, we include them in our regression and notice a significantly \ 
increased relationship of 77%.

```{r, include=FALSE}
#Weighted least squares
w = 1:dim(bike)[1]

rf = rstandard(model3)
s.hat = abs(rf)
rf2 = lm(s.hat~.-Date, data=bike[-c(2)])
var.s = (predict(rf2))^2

model7 <- lm(I(`Rented Bike Count`^0.1818) ~ .-Date, data = bike, weights=w)
model7$call
summary(model7)$r.squared
summary(model7)$adj.r.squared

# Fitted vs residuals
#ggplot(model3, aes(x = .fitted, y = .resid)) + geom_point()

# QQ plot against residuals
#ggplot(model3, aes(sample = .resid)) + stat_qq() + stat_qq_line()
```

\ 
We go further and test a model of __Weighted least squares__ paired with a __transformation__\ 
and notice strong relationship of 83% between our response variable and the selected\ 
explanatory variables.\ 

## MODEL SELECTION
We have made many models using various ways, however there is still an automated \ 
test we can use for model selection which tests different subsets of models, \ 
comparing each with different values, which are $R^2$, $R_{adj}^2$, $cp$,$AIC$.\ 
The aim of this automation is to pick a value that __maximizes__ the $R^2$ and $R_{adj}^2$\ 
and __minimizes__ $AIC$. These thresholds usually tend to provide the best possible model.

```{r, include=FALSE}
#Subset selection
library(leaps)
allreg <- regsubsets(I(`Rented Bike Count`^0.1818) ~ .-Date, nbest=8, data=bike)
n = dim(bike)[1]
aprout = summary(allreg)
pprime = apply(aprout$which, 1, sum)
aprout$aic <- aprout$bic - log(n) * pprime + 2 * pprime
df = data.frame(with(aprout, round(cbind(which, rsq, adjr2, cp, bic, aic), 3)))
df[which.max(df$aic), ] #Model with lowest AIC
df[which.max(df$adjr2), ] #Model with highest adjusted R^2
```
After we run our algorithm we finally obtain the _best possible predictors_ \ 
however as we can see the $R^2$ has not improved much..\ 


```{r, include=FALSE}
library(MASS)
library(stats)
library(MPV)
#step1 = stepAIC(model4, direction="both")
```

Another automated testing alogrithm is __backward elimination__ which starts of\ 
with a complex model and reduces model complexity by testing relevant features\ 
the R output shows the same result as the fully automated test above.

```{r, message=FALSE, warning=FALSE}
step2 = stepAIC(model3, direction="backward")
pprime2 = step2$rank
n = dim(bike)[1]
aic = step2$anova[6][1]
bic = aic + log(n) * pprime2 - 2 * pprime2
bic
PRESS(model3)

#DFFITS
arr = dffits(model3)
print(length(which(arr > 1)))

#DFBETAS
arr1 = dfbetas(model3)
print(length(which(arr > 1)))

#COOK'S DISTANCE
library(car)
arr2 = cooks.distance(model3)
print(length(which(arr > qf(0.2, pprime2, n-pprime2))))
```

### TRAIN TEST SPLIT
We need to validate our model, and to do so we need to create a training set and a test set. We split our initial dataset into a training set and a test set with proportion 80:20. 

```{r, include=FALSE}
create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
```
```{r}
data_train <- create_train_test(bike, size = 0.8, train = TRUE)
data_test <- create_train_test(bike, size = 0.8, train = FALSE)
```

```{r, include=FALSE}
model7_training <- lm(I(`Rented Bike Count`^0.1818) ~ ., data = data_train[, -c(1)], weight = 1:dim(data_train)[1]) # excluding date
#model6_training <- lm(I(`Rented Bike Count`^0.1818) ~ .-`Temperature(°C)`-`Dew point temperature(°C)`-`Visibility (10m)`-`Snowfall (cm)`-`Wind speed (m/s)`, data = data_train[, -c(1)]) # excluding date
mean(model7_training$residuals^2)
#summary(model6_training)
model7_pred <- predict.lm(model7_training, data_test[,-c(1)]) # excluding date, rented bike
mspr2 = mean((data_test$`Rented Bike Count` - model7_pred)^2)
print(mspr2)
print(PRESS(model7_training))

#This is to compare with ML models
RMSE(data_test$`Rented Bike Count`, model7_pred)
baseline_r2 = cor(data_test$`Rented Bike Count`, model7_pred) ^ 2
```

We now run the model validation for our best performing multiple regression model. First, we would fit the model on the training set, and use the model to make the prediction. We would then compute the MSE and MSPR of the model to evaluate the predictive power. Here, we can observe a huge discrepancy between the MSE and MSPR, which shows that there might be an important feature missing in the dataset. We then calculate RMSE and $R^2$ for our prediction to compare with other models.
```{r}
model6_training <- lm(I(`Rented Bike Count`^0.1818) ~ .-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`, data = data_train[, -c(1)]) # excluding date
#MSE
mean(model6_training$residuals^2)
#summary(model6_training)
model6_pred <- predict.lm(model6_training, data_test[,-c(1)]) # excluding date, rented bike
mspr = mean((data_test$`Rented Bike Count` - model6_pred)^2)
print(mspr)
print(PRESS(model6_training))
#This is to compare with ML models
baseline_rmse = RMSE(data_test$`Rented Bike Count`, model6_pred)
baseline_r2 = cor(data_test$`Rented Bike Count`, model6_pred) ^ 2
```
## Machine Learning Models

### Decision Tree
We would first fit the decision tree regression to our data. The final $R^2$ is 0.4055, which is not so good.
```{r, include=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
normalize <- function(x, na.rm = TRUE) {
    return((x- min(x)) /(max(x)-min(x)))
}
n_data_train <- (data_train %>% mutate(across(where(is.numeric), normalize)))
n_data_test <- (data_test %>% mutate(across(where(is.numeric), normalize)))
```

```{r}
decisiontree <- rpart(`Rented Bike Count`~.-Date-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`, data = data_train, method = 'anova')
pred = predict(decisiontree, data_test, method = "anova")

#RMSE
dt_rmse = RMSE(pred = pred, obs = data_test$`Rented Bike Count`)
#R^2
dt_r2 = cor(data_test$`Rented Bike Count`, pred) ^ 2
```

### Random Forest
Next, we would try to fit the random forest model. This model is an ensemble of decision tree, and we would expect the result to be better than decision tree. Indeed, we can observe that the $R^2$ is 0.74 and the RMSE is quite small.
```{r, include=FALSE}
library(randomForest)
data_train2 <- data_train
data_test2 <- data_test
names(data_train2) <- make.names(names(data_train2))
names(data_test2) <- make.names(names(data_test2))
set.seed(222)
```

```{r}
random_forest <- randomForest(`Rented.Bike.Count`~.-Date-`Dew.point.temperature..C.`-`Solar.Radiation..MJ.m2.`-`Snowfall..cm.`, data=data_train2, na.action=na.exclude)
p2 <- predict(random_forest, data_test2)
rf_rmse = RMSE(pred = p2, obs = data_test2$`Rented.Bike.Count`)
#R^2
rf_r2 = cor(data_test2$`Rented.Bike.Count`, p2) ^ 2
```

### XGBoost
We would now try extreme gradient boosting regression, a famous model using in many competitions in Kaggle. The final result is comparable to random forest, with $R^2 =0.71$ and RMSE is also acceptable.
```{r, include=FALSE}
library(xgboost)
xgb_train = xgb.DMatrix(data = data.matrix(subset(data_train, select = -c(Seasons, `Rented Bike Count`, Date, `Dew point temperature(°C)`, `Solar Radiation (MJ/m2)`, `Snowfall (cm)`))), label = data_train$`Rented Bike Count`)
xgb_test = xgb.DMatrix(data = data.matrix(subset(data_test, select = -c(Seasons, `Rented Bike Count`, Date, `Dew point temperature(°C)`, `Solar Radiation (MJ/m2)`, `Snowfall (cm)`))), label = data_test$`Rented Bike Count`)
```

```{r, echo=TRUE, results="hide"}
xgbr = xgboost(data=xgb_train, max.depth = 2, nrounds=50)
pred2 = predict(xgbr, xgb_test)
xgbr_rmse = RMSE(pred2, data_test$`Rented Bike Count`)
# R^2
xgbr_r2 = cor(data_test2$`Rented.Bike.Count`, pred2) ^ 2
```

### Support Vector Regression
We would now fit the support vector regression model. The final result is not good with $R^2 = 0.58$. One rationale for this problem is that we are using the default kernel for SVM. We might need to find another kernel that works well for our particular dataset.
```{r, include=FALSE}
#Load Library
library(e1071)
set.seed(222)

#Regression with SVM
n1_data_train = subset(n_data_train, select = -c(Seasons))
```

```{r}
modelsvm <- svm(`Rented Bike Count`~.-Date-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`, data = n1_data_train, type="eps-regression")
#Predict using SVM regression
predictions = predict(modelsvm, subset(n_data_test, select = -c(Seasons, `Rented Bike Count`)))

svr_rmse = RMSE(predictions * (max(data_test$`Rented Bike Count`) - min(data_test$`Rented Bike Count`)) + min(data_test$`Rented Bike Count`), data_test$`Rented Bike Count`)
#R^2
svr_r2 = cor(data_test$`Rented Bike Count`, predictions* (max(data_test$`Rented Bike Count`) - min(data_test$`Rented Bike Count`)) + min(data_test$`Rented Bike Count`)) ^ 2
```
### LightGBM
LightGBM seems to be our best performing ML model with $R^2 = 0.76$ and $RMSE = 324$. This is the highest $R^2$ value and smallest $RMSE$ value for all of our models. It seems like all the tree-based algorithm, such as random forest, XGBoost or LightGBM works really well for our data.
```{r, include=FALSE}
library(lightgbm)
X=data.matrix(subset(data_train, select = -c(`Rented Bike Count`, Date, `Dew point temperature(°C)`, `Solar Radiation (MJ/m2)`, `Snowfall (cm)`)))
y=data.matrix(data_train[, names(data_train) == "Rented Bike Count"])
X_test=data.matrix(subset(data_test, select = -c(`Rented Bike Count`, Date, `Dew point temperature(°C)`, `Solar Radiation (MJ/m2)`, `Snowfall (cm)`)))
y_test=data.matrix(data_test[, names(data_test) == "Rented Bike Count"])
```

```{r include=FALSE}
dtrain <- lgb.Dataset(X, label = y)
dtest <- lgb.Dataset.create.valid(dtrain, X_test, label = y_test)
```

```{r message=FALSE, warning=FALSE, results="hide", echo=TRUE}
model <- lightgbm(params = list(objective = "regression", metric = "l2"), data = dtrain)
lgbmpred = predict(model, X_test)
```
```{r}
lgbm_rmse = RMSE(pred = lgbmpred, obs = data_test$`Rented Bike Count`)
#R^2
lgbm_r2 = cor(data_test$`Rented Bike Count`, lgbmpred) ^ 2
```

### KNN
In the following model, we use K-Nearest Neighbors to predict the bike count. We can see that the result isn't stellar ($R^2 = 0.53$), and RMSE seems to have a fairly large value. One reason is that our data has many features, which means it has high dimensional structure, and KNN does not work well under this setting.
```{r, include=FALSE}
library(caret)
X2=data.matrix(subset(n_data_train, select = -c(`Rented Bike Count`, Date, `Dew point temperature(°C)`, `Solar Radiation (MJ/m2)`, `Snowfall (cm)`)))
y2=data.matrix(n_data_train[, names(n_data_train) == "Rented Bike Count"])
X_test2=data.matrix(subset(n_data_test, select = -c(`Rented Bike Count`, Date, `Dew point temperature(°C)`, `Solar Radiation (MJ/m2)`, `Snowfall (cm)`)))
y_test2=data.matrix(n_data_test[, names(n_data_test) == "Rented Bike Count"])
```

```{r}
knnmodel = knnreg(X2, y2)
pred_y = predict(knnmodel, X_test2)
knn_rmse = RMSE(data_test$`Rented Bike Count`, pred_y * (max(data_test$`Rented Bike Count`) - min(data_test$`Rented Bike Count`)) + min(data_test$`Rented Bike Count`))
#R^2
knn_r2 = cor(data_test$`Rented Bike Count`, pred_y * (max(data_test$`Rented Bike Count`) - min(data_test$`Rented Bike Count`)) + min(data_test$`Rented Bike Count`)) ^ 2
```

### Neural Network
We use a one-layer neural network on our dataset. The result is not good for the neural network, which is quite surprising for us. In general, neural network does not work well for sparse data, since we have some columns with a lot of zeros. Moreover, it seems like neural network is a better fit for classification tasks instead of regression tasks.
```{r, include=FALSE}
library(nnet)
set.seed(222)
data_train3 <- n_data_train[, -c(1)] # exclude date
data_test3 <- n_data_test[, -c(1)] # exclude date
names(data_train3) <- make.names(names(data_train3))
names(data_test3) <- make.names(names(data_test3))
data_train3 <- data.matrix(data_train3)
data_test3 <- data.matrix(data_test3)
```

```{r, echo = TRUE, results="hide"}
nn <- nnet(`Rented.Bike.Count`~.-`Dew.point.temperature..C.`-`Solar.Radiation..MJ.m2.`-`Snowfall..cm.`, data = data_train3, linout = FALSE, size=50, maxit = 1000, rang=0.05, decay=5e-4, trace = TRUE)

nnpred = predict(nn, data_test3)
```

```{r}
nn_rmse = RMSE(data_test$`Rented Bike Count`, nnpred * (max(data_test$`Rented Bike Count`) - min(data_test$`Rented Bike Count`)) + min(data_test$`Rented Bike Count`))
#R^2
nn_r2 = cor(data_test$`Rented Bike Count`, nnpred * (max(data_test$`Rented Bike Count`) - min(data_test$`Rented Bike Count`)) + min(data_test$`Rented Bike Count`)) ^ 2
```

Comparision for the predictive performance of our models:

```{r, echo=FALSE}
rmse_lst = c(baseline_rmse, dt_rmse, rf_rmse, svr_rmse, lgbm_rmse, xgbr_rmse, knn_rmse, nn_rmse)
r2_lst = c(baseline_r2, dt_r2, rf_r2, svr_r2, lgbm_r2, xgbr_r2, knn_r2, nn_r2)
rnames = c("Baseline model", "Decision Tree", "Random Forest", "Support Vector Regression", "LightGBM Regression", "XGBoost Regression", "KNN Regression", "One-layer Neural Network")
tab = data.frame(rnames, rmse_lst, r2_lst)
names(tab) <- c("Model", "RMSE", "R-Squared")

pander(tab)
```

We can observe that the predictive power of the machine learning models is much greater than that of our baseline multiple linear regression model. Especially, tree-based algorithm such as random forest and gradient boosting algorithm works especially well for our particular dataset.

### Discussion/Conclusion
The optimal model we ended up was a simple additive model with a transformed response variable. With it we found that Wind Speed, Holiday, Visibility, Seasons, Rainfall, Temperature, Hour, and Functioning Day are significant and good predictors. The other variables that we dropped do not help with prediction with our model. There are limitations because of the discrepancy between the low MSRP and MSE of our model. Furthermore, we see with how the machine learning models only hovering around a 0.8 R-squared, there may be missing data features. Going further we may explore time series given the rented bike count plot, as well as ridge regression since it helps solve the problems of multicollinearity in a general sense for regression. Additionally, we could also explore cyclic transformations on the Hour variable or take it as a categorical variable.

### References
E, S. V., Park, J., & Cho, Y. (2020). Using data mining techniques for bike sharing demand prediction in Metropolitan City. Computer Communications, 153, 353–366. https://doi.org/10.1016/j.comcom.2020.02.007 

E, S. V., & Cho, Y. (2020). A rule-based model for Seoul bike sharing demand prediction using weather data. European Journal of Remote Sensing, 53(sup1), 166–183. https://doi.org/10.1080/22797254.2020.1725789 

E, S. V., Park, J., & Cho, Y. (2020). Seoul bike trip duration prediction using data mining techniques. IET Intelligent Transport Systems, 14(11), 1465–1474. https://doi.org/10.1049/iet-its.2019.0796 
